As discussed in chapter 8, solutions to the issues of distributed systems, assuming they actually can be detected, usually lie in the direction of various abstract guarantees. These tend to be broadly classified as "consistency guarantees", especially in the database domain. 
A very popular model is linearizability - being able to work with the system as though all of the transactions are happening in the same order. In other words, in a linearizable system, the concept of concurrency does not exist. It makes the system extremely easy to reason about, but implementing linearizability is associated with serious fault-tolerance problems, since the most typical way to do this is for single-leader systems: in cases where network delay is significant, clients that cannot reach the leader (or, more realistically, reach the leader before the timeout) cannot write and can only read potentially stale data. The rule of thumb is usually defined as the CAP theorem: "Consistency (in this case - **and only in this case** - equivalent to Linearizability), Availability or Partition (network failure) tolerance", where the idea is that only two of these three features can be chosen. However, this phrasing is misleading because one usually does not get to choose whether a network parition occurs. A better way to use the CAP is "**(linearizable)** Consistency or Availability **when Paritioned**". It is also important to mention that it's rare for a network parition to be clearly defined: e.g. one must still make decisions about how to decide when a node is dead, how to decide what to do about network delays (do we consider them a partition?), etc. Linearizability also has a cost in performance - this is discussed in Ch7 on the example of strongly isolated transactions.

A weaker model is causal consistency, which does not guarantee a totally valid order of events (some are understood to be concurrent), but preserves a logical order between events. Concurrency in this system may still exist: different branches of history (much like a Git commit history) are possible. This allows to sidestep the restriction of the CAP theorem. This can be achieved with Lamport timestamps - `(counter, node_id)`, where the counter is monotonically increasing for every time the node updates a given key in its data. This can help resolve conflicts eventually with no impact from e.g. clock skew, however, this cannot help resolve conflicts on-the-fly: if a given node receives a request to create a new some key, it has, at the moment of deciding how to process this request, no way of knowing whether another node is currently trying to process a conflicting request for the same key (assuming that keys must be unique). To achieve that, consensus is needed. 

A very large amount of problems in distributed systems can be reduced to consensus between nodes:
  - Atomic transaction commits in databases
  - Ordering of messages in messaging systems
  - Locks and leases in highly contented systems
  - Split-brain-avoidant failure detection
  - Records with uniqueness constraints

All of these problems do not exist in systems with a single node. A number of distributed systems opt for maintaining the same ease, by having single-leader distributed systems. This imposes a cost in terms of making situations when the leader has failed or is unreachable due to network failure hard to deal with. Several approaches exist:
  - Simply waiting for the leader to recover, and accept that the system is going to be unreachable unless the leader recovers.
  - Manual failover by requiring the human operator to appoint a new leader. This imposes a cost in terms of human capital, as for important systems this requires that an on-call system administrator be always available.

Ridding a leader-driven system of these problems (which is important if the system is to be highly available) requires using an algorithm to choose a new leader automatically. This, again, requires consensus. So, in a way, for highly available distributed systems with no conflicts, consensus is inescapable. There are a number of algorithms for achieving it, and most implementers of distributed systems should not try to implement their own: using proven tools like Apache Zookeeper is advised instead. It is not typical for application developer to use Zookeeper by itself: typically, systems like HBase or Kafka use it under the hood.

If a system has to be highly available but can accept the possibility of conficts, a leaderless/multi-leader system can be used, these normally do not rely on consensus. In such cases, the application must implement some custom conflict resolution logic.
