There are generally several approaches to managing computationally intense workloads:
- HPC(High-performance computing): a very large and powerful single machine, with massive amount of CPUs, RAM, etc, with nodes being physically close and internal communication being trivial and safely assumed to be reliable.
- Distributed computing: relying on a network of multiple nodes to split workloads.
In less extreme forms, these map onto the concepts of vertical and horizontal scaling, which has to do with dynamics of system evolution to improve capacity (either switch from a relatively weak single machine to a relatively strong single machine, or add a number of relatively weak machines).
The tradeoff with vertical scaling (the radical ultimate form of which are HPC supercomputers) is the escalating cost of more performant hardware, which at supercomputer level is extremely high. In comparison, distributed computing allows one to use commodity-grade machines, and is much cheaper to scale. It also allows to leverage geographical distribution (an important feature for global systems like CMS), and are theoretically more fault tolerant (if something happens to the single HPC machine, the entire system will be brought down). The downside is a non-trivial amount of reliability issues with distributed systems, inherent in their distributed nature.

Issues of distributed systems normally stem from that they are "shared-nothing", meaning that they rely on a network to exchange messages/data. Networks are notoriously less stable than single systems. Some issues stemming from network usage are:
- Packet loss/arbitrarily long delay that the sender has no information about. 
- Significantly out-of-sync clocks with unpredictable behavior when synchronized (may have signinficant jumps). This is compounded by the fact that clock systems, in an overwhelming majority of cases, do not include confidence intervals about the values they are reporting, even if the values are extremely precise.
- A process may be paused (by something like stop-the-world garbage collection), causing the node to be considered "dead" by other nodes, and then come back alive without any information that the node is now dead.
The biggest problem with all of these is that the behavior of the system is not deterministic, and it is very hard to reproduce the issues stemming from network issues. 
	
Things described above are considered "partial failures": only some nodes are failing, but in a significant way that affects the system overall. This is in comparison to single machines (including HPC) where a serious issue is typically causing a total stop of the entire computer. This is an intentional feature of computer design: if it works, the results can be generally expected to be correct. Engineers working on distributed systems, on the other hand, have to build them in a way that is tolerant of partial failures.  

Firs issue with handling failures is detection. It is impossible to know what exactly went wrong with a request that never received a response - maybe the target node died, maybe network transmission slowed down extremely. 
Here, it is valuable to understand different models of networks:
- Synchronous networks allow assumptions about how much a network delay can be, how long process pauses may be, and how much clocks may drift. It is completely unrealistic for practical Internet systems. However, they are realistic for e.g. phone networks: when a phone call is placed, a circuit of guaranteed bandwidth is estabilished (e.g. ISDN protocol) between the callers, and is kept no matter how much data is actually transmitted.
- Asynchronous networks allow absolutely no assumptions to be made, not even about time tracking, so even timeouts cannot be relied upon. This is an extremely pessimistic scenario and some algorithms are implemented for it, but is extremely restrictive.
- Partially synchronous networks behave as synchronous most of the time, but exceed the bounds sometimes. This is a realistic model for most Internet systems. 
Some cases allow some information about the network to be gauged in case of partial failures:
- If the machine is up but the target port is not listened on, its OS will respond with a refuse (RST) or close (FIN) TCP connections. However, if this happens in between a long transmission, no information about how much data was processed before the failure is available.
- If the process on the node crashes but the OS is still functional, a script may exist to alert other system nodes to the failure.
- If the network router is aware of the IP of the target being unreachable it may return ICMP Destination Unreachable packet.
- If the user has access to the physical infrastructure of the network, machines being unavailable may be checked on the network switches (however, for cloud services this is likely impossible).
In all other cases, the only way (assuming the system is at least partially synchronous and timeouts can be used) to handle network failures is setting a connection timeout. However, since delays in partially synchronous systems can be unbounded, and both too long and too short timeouts are dangerous (too long and the failure detection is too slow, too short and the system performs unnecessary failovers), the only way is to set them experimentally by measuring the behavior of the system under expected load. 

The handling part is also hard. Since shared-nothing machines by definition cannot access anything truly shared, there is no way for a single node to reliably know the state of all other nodes in the system. The unreliable network is the only source of communication, and major decisions about the network are usually made by consensus. 
